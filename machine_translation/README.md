## Neural Machine Translation using Seq2Seq with Attention Mechanism

You can follow links below for more details on implementation:
- [Neural Machine Translation With Tensorflow: Data Preparation](https://machinetalk.org/2019/02/20/neural-machine-translation-with-tensorflow-data-preparation/)
- [Neural Machine Translation With Tensorflow: Model Creation](https://machinetalk.org/2019/02/27/neural-machine-translation-with-tensorflow-model-creation/)
- [Neural Machine Translation With Tensorflow: Training](https://machinetalk.org/2019/03/05/neural-machine-translation-with-tensorflow-training/)
- Neural Machine Translation With Tensorflow: Inference (in progress)
- Neural Machine Translation With Tensorflow: Attention (in progress)

### Reference
- Tensorflow's NMT repository: [link](https://github.com/tensorflow/nmt)
- Sequence to Sequence Learning with Neural Networks paper: [link](https://arxiv.org/abs/1409.3215)
- Effective Approaches to Attention-based Neural Machine Translation paper: [link](https://arxiv.org/abs/1508.04025)
- Pytorch's Seq2Seq Tutorial: [link](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)
- This repo is forked: [link](https://github.com/ChunML/NLP)
